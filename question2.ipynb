{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement Logistic Regression from scratch in this question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You are provided with the dataset of sign language digits. Implement logistic regression from scratch to classify the images provided in the dataset. Load the dataset and perform splitting into training and test sets with 70:30 ratio randomly using test train split.\n",
    "2. Plot a diagram for the sigmoid function. This is used for binary classi\u0000cation. How do you modify it for multilabel dataset classification problems? State and Explain the methods used.\n",
    "3. Use both one vs all and one vs one method for the above problem statement purpose.\n",
    "4. Also get results using Log Reg from scikit learn.\n",
    "5. Report accuracy score, Confusion matrix and any other metrics you feel useful and Compare the results - from all the three.\n",
    "\n",
    "\n",
    "\n",
    "[BONUS]\n",
    "6. Display few pictures with their predicted and original labels \n",
    "7. Do the results differ? State the reasons why it is so.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset link : \n",
    "https://iiitaphyd-my.sharepoint.com/:f:/g/personal/apurva_jadhav_students_iiit_ac_in/Eictt5_qmoxNqezgQQiMWeIBph4sxlfA6jWAJNPnV2SF9Q?e=mQmYN0 \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score,confusion_matrix,r2_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_l = np.load(\"X.npy\") # image\n",
    "y_l = np.load(\"Y.npy\") # label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_l = x_l.reshape(2062, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-5, 5, 100) \n",
    "z = 1/(1 + np.exp(-x)) \n",
    "  \n",
    "plt.plot(x, z) \n",
    "plt.xlabel('x') \n",
    "plt.ylabel('Sigmoid(x)')\n",
    "plt.title('Graph of sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "**Logistic regression** can only be used for two class problems where the output label is binary like a **yes** or **no**, **true** or **false** case. To use Logistic Regression for multiple classes there are two methods that we can use :-\n",
    "1. **One Vs One**: in this method we need to consider $K \\choose 2$ classifiers *(K = number of classes)* one for each pair of classes. To build it we need to extract only those data point *(samples)* belonging to these classes. For testing we need to use all the $K \\choose 2$ classifiers and count the frequencies for each class. Finally we predict the label *(class)* with the maximum frequency.\n",
    "2. **One Vs All**: in this method we need to consider $K$ classifiers one for each class. For each classifier the data belonging to that class is labeled as 1 and everything else as 0, which converts it into a two class *(binary)* problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, train_data, Y):\n",
    "        self.data = train_data  # It is assumed that data is normalized and shuffled (rows, cols)\n",
    "        self.Y = Y[:, np.newaxis]\n",
    "        self.b = np.random.randn()\n",
    "        self.cols = self.data.shape[1]\n",
    "        self.rows = self.data.shape[0]\n",
    "        self.weights = np.random.randn(self.cols, 1)  # Initialising weights to 1, shape (cols, 1)\n",
    "        self.num_iterations = 500\n",
    "        self.learning_rate = 0.0001\n",
    "        self.batch_size = 30\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "        \n",
    "    def calc_mini_batches(self):\n",
    "        new_data = np.hstack((self.data, self.Y))\n",
    "        np.random.shuffle(new_data)\n",
    "    \n",
    "        rem = self.rows % self.batch_size\n",
    "        num = self.rows // self.batch_size\n",
    "        till = self.batch_size * num\n",
    "        if num > 0:\n",
    "            dd = np.array(np.vsplit(new_data[ :till, :], num))\n",
    "            X_batch = dd[:, :, :-1]\n",
    "            Y_batch = dd[:, :, -1]\n",
    "            \n",
    "        return X_batch, Y_batch\n",
    "\n",
    "    def update_weights(self, X, Y):\n",
    "        Y_predicted = self.predict(X) # Remember that X has data stored along the row for one sample\n",
    "        gradient = np.dot(np.transpose(X), Y_predicted - Y)\n",
    "        self.b = self.b - np.sum(Y_predicted - Y)\n",
    "        self.weights = self.weights - (self.learning_rate * gradient) # vector subtraction\n",
    "    \n",
    "    def print_error(self):\n",
    "        Y_Predicted = self.predict(self.data)\n",
    "        class_one = self.Y == 1\n",
    "        class_two = np.invert(class_one)\n",
    "        val = np.sum(np.log(Y_Predicted[class_one]))\n",
    "        val += np.sum(np.log(1 - Y_Predicted[class_two]))\n",
    "        print(-val)\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        for j in range(self.num_iterations):\n",
    "            X, Y = self.calc_mini_batches()\n",
    "            num_batches = X.shape[0]\n",
    "            for i in range(num_batches):\n",
    "                self.update_weights(X[i, :, :], Y[i, :][:, np.newaxis])  # update the weights\n",
    "            if (j+1)%100 == 0:\n",
    "                self.print_error()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # X is 2 dimensional array, samples along the rows\n",
    "        return self.sigmoid(np.dot(X, self.weights) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1165.6003822929379\n",
      "728.3022847219468\n",
      "897.434189968287\n",
      "718.1614823861232\n",
      "541.7357048510157\n",
      "accuracy : 0.8933764135702746\n",
      "f1 score : 0.5875\n",
      "1125.22289014278\n",
      "507.0089105969695\n",
      "389.82620626449386\n",
      "332.15433633560974\n",
      "344.6483942547981\n",
      "accuracy : 0.9079159935379645\n",
      "f1 score : 0.6013986013986015\n",
      "1387.912459129495\n",
      "830.1712256267981\n",
      "678.2564383728258\n",
      "828.8724523414282\n",
      "585.7797020836076\n",
      "accuracy : 0.9159935379644588\n",
      "f1 score : 0.4090909090909091\n",
      "1100.0230808978768\n",
      "1219.198878086387\n",
      "670.2652729785949\n",
      "516.3086532919759\n",
      "462.875285611696\n",
      "accuracy : 0.8723747980613893\n",
      "f1 score : 0.40601503759398494\n"
     ]
    }
   ],
   "source": [
    "log_regressors_ova = []\n",
    "for i in range(4):\n",
    "    mask = y_l[:,i] >= 1.0 - 1e-6\n",
    "    others = np.invert(mask)\n",
    "    x_pos = x_l[mask]\n",
    "    x_neg = x_l[others]\n",
    "    y_pos = [1]*len(x_pos)\n",
    "    y_neg = [0]*len(x_neg)\n",
    "    y_new = y_pos + y_neg\n",
    "    y_new = np.array(y_new)\n",
    "    x_new = np.vstack((x_pos, x_neg))\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.30, shuffle=True)\n",
    "    reg = MyLogisticRegression(x_train, y_train)\n",
    "    reg.gradient_descent()\n",
    "    y_pred = reg.predict(x_test)\n",
    "    pred = y_pred >= 0.5\n",
    "    pred = pred.astype(int)\n",
    "    print('accuracy : {a}'.format(a=accuracy_score(y_test, pred)))\n",
    "    print('f1 score : {a}'.format(a = f1_score(y_test, pred)))\n",
    "    log_regressors_ova.append(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9644588045234249\n",
      "f1 score : 0.8070175438596492\n",
      "accuracy : 0.9741518578352181\n",
      "f1 score : 0.8596491228070176\n",
      "accuracy : 0.9192245557350566\n",
      "f1 score : 0.5535714285714286\n",
      "accuracy : 0.9353796445880452\n",
      "f1 score : 0.6153846153846153\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "for i in range(4):\n",
    "    mask = y_l[:,i] >= 1.0 - 1e-6\n",
    "    others = np.invert(mask)\n",
    "    x_pos = x_l[mask]\n",
    "    x_neg = x_l[others]\n",
    "    y_pos = [1]*(x_pos.shape[0])\n",
    "    y_neg = [0]*(x_neg.shape[0])\n",
    "    y_new = y_pos + y_neg\n",
    "    y_new = np.array(y_new)\n",
    "    x_new = np.vstack((x_pos, x_neg))\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.30, shuffle=True)\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    print('accuracy : {a}'.format(a=accuracy_score(y_test, pred)))\n",
    "    print('f1 score : {a}'.format(a = f1_score(y_test, pred)))\n",
    "    log_regressors_ova.append(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.18939498695491\n",
      "72.17231030451674\n",
      "67.88859433188675\n",
      "40.25629449357726\n",
      "42.597256494975284\n",
      "accuracy : 0.943089430894309\n",
      "f1 score : 0.9465648854961831\n",
      "473.81108211519916\n",
      "248.75873266800244\n",
      "243.98164297293656\n",
      "141.059886483292\n",
      "109.35743965364318\n",
      "accuracy : 0.8048780487804879\n",
      "f1 score : 0.8064516129032259\n",
      "367.39995504803807\n",
      "212.59565219221435\n",
      "157.6712682819209\n",
      "128.88749352247106\n",
      "106.63371395983012\n",
      "accuracy : 0.9032258064516129\n",
      "f1 score : 0.9\n"
     ]
    }
   ],
   "source": [
    "log_regressors_ovo = []\n",
    "for i in range(3):\n",
    "    for j in range(i+1,3):\n",
    "        mask1 = (y_l[:, i] >= 1.0 - 1e-6)\n",
    "        mask0 = (y_l[:, j] >= 1.0 - 1e-6)\n",
    "        x_pos = x_l[mask1]\n",
    "        x_neg = x_l[mask0]\n",
    "        y_pos = [1]*(x_pos.shape[0])\n",
    "        y_neg = [0]*(x_neg.shape[0])\n",
    "        y_new = y_pos + y_neg\n",
    "        y_new = np.array(y_new)\n",
    "        x_new = np.vstack((x_pos, x_neg))\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.30, shuffle=True)\n",
    "        reg = MyLogisticRegression(x_train, y_train)\n",
    "        reg.gradient_descent()\n",
    "        y_pred = reg.predict(x_test)\n",
    "        pred = y_pred >= 0.5\n",
    "        pred = pred.astype(int)\n",
    "        print('accuracy : {a}'.format(a=accuracy_score(y_test, pred)))\n",
    "        print('f1 score : {a}'.format(a = f1_score(y_test, pred)))\n",
    "        log_regressors_ovo.append(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "for i in range(3):\n",
    "    for j in range(i+1, 3):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
